{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "U9JH_uj7O4gK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented a decision tree learner for this particular problem that can derive decision trees with an arbitrary, pre-determined depth (up to the maximum depth where all data sets at the leaves are pure) using the information gain criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mJtnFgIyN2ku"
   },
   "outputs": [],
   "source": [
    "def check_purity(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    unique_classes = np.unique(label_column)\n",
    "\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def create_leaf(data, ml_task):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    if ml_task == \"regression\":\n",
    "        leaf = np.mean(label_column)\n",
    "        \n",
    "    else:\n",
    "        unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "        index = counts_unique_classes.argmax()\n",
    "        leaf = unique_classes[index]\n",
    "    \n",
    "    return leaf\n",
    "\n",
    "def get_potential_splits(data):\n",
    "    \n",
    "    potential_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "    for column_index in range(n_columns - 1): # excluding the last column which is the label\n",
    "        values = data[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "        \n",
    "        unique_values = np.sort(unique_values)\n",
    "        arr=[]\n",
    "        for i in range(len(unique_values)-1):\n",
    "            arr.append((unique_values[i] + unique_values[i+1])/2)\n",
    "        unique_values = arr\n",
    "        \n",
    "        \n",
    "        \n",
    "        potential_splits[column_index] = unique_values\n",
    "    \n",
    "    return potential_splits\n",
    "\n",
    "\n",
    "def calculate_entropy(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "     \n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calculate_mse(data):\n",
    "    actual_values = data[:, -1]\n",
    "    if len(actual_values) == 0:   \n",
    "        mse = 0\n",
    "        \n",
    "    else:\n",
    "        prediction = np.mean(actual_values)\n",
    "        mse = np.mean((actual_values - prediction) **2)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "\n",
    "def calculate_overall_metric(data_below, data_above, metric_function):\n",
    "    \n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    overall_metric =  (p_data_below * metric_function(data_below) \n",
    "                     + p_data_above * metric_function(data_above))\n",
    "    \n",
    "    return overall_metric\n",
    "\n",
    "\n",
    "def determine_best_split(data, potential_splits, ml_task):\n",
    "    \n",
    "    first_iteration = True\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "            \n",
    "            if ml_task == \"regression\":\n",
    "                current_overall_metric = calculate_overall_metric(data_below, data_above, metric_function=calculate_mse)\n",
    "            \n",
    "            # classification\n",
    "            else:\n",
    "                current_overall_metric = calculate_overall_metric(data_below, data_above, metric_function=calculate_entropy)\n",
    "\n",
    "            if first_iteration or current_overall_metric <= best_overall_metric:\n",
    "                first_iteration = False\n",
    "                \n",
    "                best_overall_metric = current_overall_metric\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value\n",
    "\n",
    "\n",
    "def split_data(data, split_column, split_value):\n",
    "    \n",
    "    split_column_values = data[:, split_column]\n",
    "\n",
    "    type_of_feature = FEATURE_TYPES[split_column]\n",
    "    if type_of_feature == \"continuous\":\n",
    "        data_below = data[split_column_values <= split_value]\n",
    "        data_above = data[split_column_values >  split_value]\n",
    "    \n",
    "    else:\n",
    "        data_below = data[split_column_values == split_value]\n",
    "        data_above = data[split_column_values != split_value]\n",
    "    \n",
    "    return data_below, data_above\n",
    "\n",
    "\n",
    "def determine_type_of_feature(df):\n",
    "    \n",
    "    feature_types = []\n",
    "    n_unique_values_treshold = 15\n",
    "    for feature in df.columns:\n",
    "        if feature != \"Class\":\n",
    "            unique_values = df[feature].unique()\n",
    "            example_value = unique_values[0]\n",
    "\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "    \n",
    "    return feature_types\n",
    "\n",
    "\n",
    "def decision_tree_algorithm(df, ml_task, counter=0, min_samples=2, max_depth=5):\n",
    "    \n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS, FEATURE_TYPES\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        FEATURE_TYPES = determine_type_of_feature(df)\n",
    "        data = df.values\n",
    "    else:\n",
    "        data = df           \n",
    "    \n",
    "    \n",
    "    if (check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "        leaf = create_leaf(data, ml_task)\n",
    "        return leaf\n",
    "\n",
    "    \n",
    "    else:    \n",
    "        counter += 1\n",
    "\n",
    "        potential_splits = get_potential_splits(data)\n",
    "        split_column, split_value = determine_best_split(data, potential_splits, ml_task)\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "        \n",
    "        if len(data_below) == 0 or len(data_above) == 0:\n",
    "            leaf = create_leaf(data, ml_task)\n",
    "            return leaf\n",
    "        \n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        type_of_feature = FEATURE_TYPES[split_column]\n",
    "        if type_of_feature == \"continuous\":\n",
    "            question = \"{} <= {}\".format(feature_name, split_value)\n",
    "            \n",
    "        else:\n",
    "            question = \"{} = {}\".format(feature_name, split_value)\n",
    "        \n",
    "        sub_tree = {question: []}\n",
    "        \n",
    "        yes_answer = decision_tree_algorithm(data_below, ml_task, counter, min_samples, max_depth)\n",
    "        no_answer = decision_tree_algorithm(data_above, ml_task, counter, min_samples, max_depth)\n",
    "        \n",
    "\n",
    "        if yes_answer == no_answer:\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        \n",
    "        return sub_tree\n",
    "\n",
    "\n",
    "def predict_example(example, tree):\n",
    "    \n",
    "    # tree is just a root node\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    \n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split(\" \")\n",
    "\n",
    "    # ask question\n",
    "    if comparison_operator == \"<=\":\n",
    "        if example[feature_name] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    \n",
    "    else:\n",
    "        if str(example[feature_name]) == value:\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    \n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return predict_example(example, residual_tree)\n",
    "\n",
    "    \n",
    "def make_predictions(df, tree):\n",
    "    \n",
    "    if len(df) != 0:\n",
    "        predictions = df.apply(predict_example, args=(tree,), axis=1)\n",
    "    else:\n",
    "\n",
    "        predictions = pd.Series()\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def calculate_accuracy(df, tree):\n",
    "    predictions = make_predictions(df, tree)\n",
    "    predictions_correct = predictions == df.Class\n",
    "    accuracy = predictions_correct.mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divided the data set into a training set comprising the first 90 data points and a test set consisting of the last 30 data elements. Used the resulting training set to derive trees of depths 1 - 8 and evaluated the accuracy of the resulting trees for the 90 training samples and for the test set containing the last 30 data items. Compared the classification accuracy on the test set with the one on the training set for each tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train=[]\n",
    "\n",
    "for i in range(1,14):\n",
    "\n",
    "    df = pd.read_csv(\"data2c.csv\")\n",
    "    tree = decision_tree_algorithm(df.iloc[:90, :], \"classification\", counter=0, min_samples=1, max_depth=i)\n",
    "    acct = calculate_accuracy(df.iloc[:90,:], tree)\n",
    "    accuracy_train.append(acct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7222222222222222,\n",
       " 0.7333333333333333,\n",
       " 0.7777777777777778,\n",
       " 0.8222222222222222,\n",
       " 0.8333333333333334,\n",
       " 0.8444444444444444,\n",
       " 0.8555555555555555,\n",
       " 0.9,\n",
       " 0.9444444444444444,\n",
       " 0.9666666666666667,\n",
       " 0.9888888888888889,\n",
       " 0.9888888888888889,\n",
       " 1.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TFwMtIKjPFxh"
   },
   "outputs": [],
   "source": [
    "accuracy=[]\n",
    "\n",
    "for i in range(1,14):\n",
    "\n",
    "    df = pd.read_csv(\"data2c.csv\")\n",
    "    tree = decision_tree_algorithm(df.iloc[:90, :], \"classification\", counter=0, min_samples=1, max_depth=i)\n",
    "    acc = calculate_accuracy(df.iloc[90:,:], tree)\n",
    "    accuracy.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7333333333333333,\n",
       " 0.7333333333333333,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.8333333333333334,\n",
       " 0.7666666666666667,\n",
       " 0.7666666666666667,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "max_depth_list = [i for i in range(1,14)]\n",
    "x = np.arange(len(max_depth_list)) + 1 \n",
    "plt.plot(x, accuracy_train)\n",
    "plt.plot(x, accuracy, label='Accuracy vs max_depth') \n",
    "plt.xlabel('Maximum Depth') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.legend() \n",
    "plt.show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification accuracy on the test vs train for each tree depth\n",
    "\n",
    "From the above graph it can be inferred that the accuracy on the test data and train keeps on increasing with the increasing values of depth till max_depth=8. From this point the test result starts decreasing and the train results keeps on increasing which indicates the overfitting of the train data on the classifier. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented a bagging routine for the decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5Xfn5JqzSt6R"
   },
   "outputs": [],
   "source": [
    "df_train = df.iloc[:90, :]\n",
    "df_test = df.iloc[90:, :]\n",
    "actual = df_test.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qYufYbHjYNRu"
   },
   "outputs": [],
   "source": [
    "def accuracy_metric(actual, predicted):\n",
    "        correct = 0\n",
    "        for i in range(len(actual)):\n",
    "            if actual[i] == predicted[i]:\n",
    "                correct += 1\n",
    "        return correct / float(len(actual)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied bagging 10, 50, and 100 times to the training data. For each of the three cases, evaluated the resulting ensemble classifier on the test data set and compared the error rates for a single classifier of the chosen depth and the three ensemble classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging=[10,50,100]\n",
    "accuracy_bagging=[]\n",
    "for bag in bagging:\n",
    "\n",
    "    predictions=[]\n",
    "    for i in range(bag):\n",
    "        indexes = np.random.randint(low=0, high=df_train.shape[0], size=df_train.shape[0])\n",
    "        dataset = df_train.iloc[indexes]\n",
    "        tree = decision_tree_algorithm(dataset, \"classification\", counter=0, min_samples=1, max_depth=8)\n",
    "        pred = make_predictions(df.iloc[90:,:-1], tree)\n",
    "        predictions.append(pred.tolist())\n",
    "\n",
    "    final_pred = []\n",
    "    # prediction = []\n",
    "    for j in range(len(predictions[0])): \n",
    "        count_M=0\n",
    "        count_W=0\n",
    "        for i in range(len(predictions)): \n",
    "            if predictions[i][j] == ' M':\n",
    "                count_M = count_M + 1\n",
    "            else:\n",
    "                count_W = count_W + 1\n",
    "\n",
    "        if count_M >= count_W:\n",
    "            final_pred.append(\" M\")\n",
    "        else:\n",
    "            final_pred.append(\" W\")\n",
    "\n",
    "    ac = accuracy_metric(actual, final_pred)\n",
    "    accuracy_bagging.append(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7333333333333333, 0.8333333333333334, 0.8666666666666667]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_bagging #For bag =10,50,100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results it can be inferred that after applying bagging the accuracy has increased as compared to that of accuracy determined from a single classifier of depth 8 (which is the optimum depth after which the result start indicating overfitting). For Bagging value of 100, I generated 100 datasets of the same dimensions as that of actual which consist of duplicate values and ran classifier of depth 8. As a result, for the bagging part the accuracy is comparatively better than that of single classifier of the chosen depth 8. The accuracy for the single classifier of depth 8 is 0.833 where as for bagging the accuracy will change each time the code is executed as data is generated randomly and the results turn out to be better than that of result of single classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented AdaBoost on top of your decision tree classifier. Applied boosting 10, 25, and 50 times to the training data. For each of the three cases, evaluated the resulting ensemble classifier on the test data set and compared the error rates for a single classifier with the chosen depth and the three ensemble classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boosting = [10, 25, 50]\n",
    "\n",
    "for iterations in n_boosting:\n",
    "\n",
    "    train_data_weighted = train_data.copy()\n",
    "    weights = np.ones(len(train_data)) / len(train_data)\n",
    "    train_data_weighted['weight'] = weights\n",
    "\n",
    "    trees_list = []\n",
    "    alpha_list = []\n",
    "    for i in range(iterations):\n",
    "\n",
    "        tree = decision_tree_algorithm(train_data, \"Classification\", counter=0, min_samples=2, max_depth=5)\n",
    "\n",
    "        test = test_data.loc[:, test_data.columns != 'weight']\n",
    "\n",
    "        train_labels = train_data_weighted['Class'].values\n",
    "        x_train = train_data_weighted.loc[:, train_data_weighted.columns != 'Class']\n",
    "        x_train = x_train.loc[:, x_train.columns != 'weight']\n",
    "\n",
    "        error_list, prediction_list = [], []\n",
    "        for index, sample in x_train.iterrows():\n",
    "            prediction = make_predictions(sample, tree)\n",
    "            prediction_list.append(prediction[0])\n",
    "            error_list.append(1 if prediction != train_labels[index] else 0)\n",
    "\n",
    "        w = train_data_weighted['weight'].values\n",
    "        epsilon = sum(w * error_list) / w.sum()\n",
    "\n",
    "        if 0 < epsilon < 0.5:\n",
    "            trees_list.append(tree)\n",
    "\n",
    "            alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n",
    "            alpha_list.append(alpha)\n",
    "            y = [1 if label == ' M' else -1 for label in train_labels]\n",
    "            h = [1 if pred == ' W' else -1 for pred in prediction_list]\n",
    "            weights = weights * np.exp(-alpha * np.asarray(y) * np.asarray(h))\n",
    "            train_data_weighted['weight'] = weights\n",
    "\n",
    "    test_labels = test_data['Class'].values\n",
    "    test_labels[test_labels == ' M'] = 1\n",
    "    test_labels[test_labels == ' W'] = -1\n",
    "\n",
    "    x_test = test_data.loc[:, test_data.columns != 'Class']\n",
    "    count_correct_pred = 0\n",
    "\n",
    "    for index, sample in x_test.iterrows():\n",
    "        classifications = [make_predictions(sample, tree) for tree in trees]\n",
    "        classifications_int = [1 if classification == ' M' else -1 for classification in classifications]\n",
    "        prediction = np.sign(sum(alpha_list * np.asarray(classifications_int)))\n",
    "        if prediction == test_labels[index]:\n",
    "            count_correct_pred += 1\n",
    "\n",
    "    accuracy = count_correct_pred / len(test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
